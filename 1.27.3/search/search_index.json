{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>ClearURLs is an add-on based on the new WebExtensions technology and is optimized for Firefox and Chrome based browsers.</p> <p>This extension will automatically remove tracking elements from URLs to help protect your privacy when browsing through  the internet. For this purpose, we use a large catalog of rules, which is actively maintained by us and the community.</p> <p>See also</p> <p>You can find more information about the structure of the rules catalog under Specifications/Rule Catalogs.</p> <p>Hint</p> <p>You can learn more information about the required permissions under Permissions.</p>"},{"location":"#application","title":"Application","text":"<p><code>https://example.com?utm_source=newsletter1&amp;utm_medium=email&amp;utm_campaign=sale</code></p> <p>Many websites use tracking elements in the URL to mark your online activity. All that tracking code is not necessary for a website to be displayed or work correctly and can therefore be removed  \u2014 that is exactly what ClearURLs does.</p> <p>Another common example are Amazon URLs. If you search for a product on Amazon you will see a very long URL, such as:</p> <pre><code>https://www.amazon.com/dp/exampleProduct/ref=sxin_0_pb?__mk_de_DE=\u00c5M\u00c5\u017d\u00d5\u00d1&amp;keywords=tea&amp;pd_rd_i=exampleProduct&amp;pd_rd_r=8d39e4cd-1e4f-43db-b6e7-72e969a84aa5&amp;pd_rd_w=1pcKM&amp;pd_rd_wg=hYrNl&amp;pf_rd_p=50bbfd25-5ef7-41a2-68d6-74d854b30e30&amp;pf_rd_r=0GMWD0YYKA7XFGX55ADP&amp;qid=1517757263&amp;rnid=2914120011</code></pre> <p>Indeed most of the above URL is tracking code. Once ClearURLs has cleaned the address, it will look like this: <code>https://www.amazon.com/dp/exampleProduct</code></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Removes tracking from URLs automatically in the background</li> <li>Blocks some common ad domains (optional)</li> <li>Has a built-in tool to clean up multiple URLs at once</li> <li>Supports redirection to the destination, without tracking services as a middleman</li> <li>Adds an entry to the context menu so that links can be copied quickly and cleanly</li> <li>Blocks hyperlink auditing, also known as ping tracking (see also this article)</li> <li>Prevents ETag tracking</li> <li>Prevents tracking injection over history API (see also: the replaceState() method)</li> <li>Prevents Google from rewriting the search results (prevents the insertion of tracking code)</li> <li>Prevents Yandex from rewriting the search results (prevents the insertion of tracking code)</li> </ul>"},{"location":"#download","title":"Download","text":""},{"location":"#donation","title":"Donation","text":"<p>Mastodon</p>"},{"location":"faq/","title":"FAQ","text":"I have suggestions or complaints. Where can I submit them? <p>If you have any suggestions or complaints, please create an issue.</p> Before a clean URL was visible, a dirty URL was briefly in the URL bar. Does ClearURLs work correctly? <p>Sometimes a dirty URL appears initially in the URL bar before a clean URL is displayed.  This does not mean that the dirty URL has been called. ClearURLs checks all requests made through the browser,  whether initiated by a user or by a background process. Each request passes through ClearURLs' filter,  which is incremental, meaning that sometimes a URL that is not yet clean is passed back to the browser.  This URL passes through the filter of ClearURLs again before the actual call and is then further cleaned in  the next step. This happens until ClearURLs make no more changes to the URL, then the URL leaves the filter of  ClearURLs. It is therefore possible that a URL that is not clean may temporarily appear in the URL bar.  To make sure that the URL is indeed cleaned correctly, you can use the integrated Cleaning Tool.  Here you can clean the resulting URL again and again until no more changes occur, then you have the final clean URL.</p> How can I quickly check if ClearURLs do work correctly? <p>You can visit the test page of ClearURLs.  If only green emojis shines on the page, ClearURLs works correctly.  Otherwise, there is an error. Please do not forget to activate JavaScript for this test page.</p> Why does ClearURLs require permission X? <p>An explanation for all required permissions can be found here: Permissions</p> For which browsers is ClearURLs available? <p>ClearURLs is currently officially supported by Mozilla Firefox, Google Chrome, and Microsoft Edge. However, it should theoretically be possible to run the addon in all Firefox- or Chrome-based browsers.</p> <p> </p> Where do I find the packed version of the addon, e.g. for debugging? <p>Here you can download the packed files for Firefox- or Chrome-based browsers: </p> <ul> <li>ClearURLs-firefox.zip</li> <li>ClearURLs-chrome.zip</li> </ul> How can I enable the logging feature in ClearURLs? <p>Here you can find a step-by-step tutorial on \u201eHow to enable logging in ClearURLs\u201c: https://www.youtube-nocookie.com/embed/Rm1YkwXQDSM</p>"},{"location":"permissions/","title":"Permissions","text":"<p>This page describes the permissions used by ClearURLs and the purpose for which they are used.</p>"},{"location":"permissions/#all_urls","title":"<code>&lt;all_urls&gt;</code>","text":"<p>This permission is needed to perform URL or URI cleaning on all pages, regardless of the protocol (HTTPS, HTTP, data, etc.).</p>"},{"location":"permissions/#webrequest-and-webrequestblocking","title":"<code>webRequest</code> and <code>webRequestBlocking</code>","text":"<p>These permissions are needed to be notified of browser requests (i.e., calling a URL/website) and interrupt them to perform any cleaning that may be necessary. The cleaned URL is then passed to the browser.</p>"},{"location":"permissions/#storage","title":"<code>storage</code>","text":"<p>This permission is needed to save settings, logs, and rules.</p>"},{"location":"permissions/#unlimitedstorage","title":"<code>unlimitedStorage</code>","text":"<p>This permission is needed to exceed any quota imposed by the <code>storage.local</code> API. The API otherwise limits us to 5MB, which would quickly be reached with the log turned on, thus rendering the addon unusable. This permission should become optional in the future.</p>"},{"location":"permissions/#contextmenus","title":"<code>contextMenus</code>","text":"<p>This permission is required for the \"Copy Clean Link Location\" function on the context menu.</p>"},{"location":"permissions/#webnavigation-and-tabs","title":"<code>webNavigation</code> and <code>tabs</code>","text":"<p>These permissions are needed to prevent websites from manipulating the URL in the location bar after the page load. ClearURLs listen for these events and undo the change. If you don't want this, you can disable this feature in the settings. To disable this feature, you have to switch the \"Prevent tracking injection over history API\" button, and then the \"Save &amp; reload\" addon button.</p>"},{"location":"permissions/#downloads","title":"<code>downloads</code>","text":"<p>This permission is required to export logs and settings. This permission should become optional in the future.</p>"},{"location":"further_readings/excluded_fields/","title":"Excluded Fields","text":"<p>Some fields are absolutely required by a page to function correctly. However, these fields may also contain tracking elements. In such cases, we only have the choice between preventing tracking and a functioning service. Below we list all the fields not included in the official rules because they have caused problems in the past.</p> Site / Service Field Description Google <code>hl</code> Required for the correct display of Google in different languages Google <code>gbv</code> Required for Google, when visiting without JavaScript Google <code>client</code> Required for asynchronous requests to the API (is filtered on Google Search) Google <code>stick</code> Required for \"People also search for\" on Google search Google <code>zx</code> Required for serveral Google services (Hangout, GMail Mailenvelope, etc.) Amazon <code>s</code> Required for sort search results by \"Featured\", \"Price: Low to High\", \"Price: High to Low\", \"Avg. Customer Review\" and \"Newest Arrivals\"  (is only not filtered on Amazons search page) Amazon <code>psc</code> Required to link to an article, which is available in different versions (e.g., color or size) Amazon <code>SubscriptionId</code> Required for subscriptions Amazon Search <code>keywords</code> Amazon search can loop without this field. See #55 Cloudflare <code>__cf_chl_jschl_tk__</code> Require by Cloudflare to verify, that an user successfully passes a IUAM/JS Challenge or Captcha Cloudflare <code>__cf_chl_captcha_tk__</code> Require by Cloudflare to verify, that an user successfully passes a IUAM/JS Challenge or Captcha backcountry.com <code>skid</code> Required to link to an article, which is available in different versions (e.g., color or size) backcountry.com <code>fskid</code> Required to link to an article, which is available in different versions (e.g., color or size) Adobe <code>cid</code> Used by Adobe for tracking, but also required by many websites for correct functioning. This rule has been temporarily removed until a whitelist feature is available TikTok <code>language</code> Is no tracking and is required for the website, e.g., the profile view Facebook <code>comment_id</code> Required for comments Facebook <code>fbid</code> Required by many different functions. Used mainly as internal ID and not for external tracking aliexpress <code>pvid</code> Required for filtering search results taobao.com <code>user_number_id</code> Required for store frontpage (unique ID for specified store) Twitter redirects <code>t</code> Is required for redirect URLs inside of Twitter mails (!21) Firefox accounts <code>context</code> Is required for Firefox Sync (!135)"},{"location":"further_readings/similar_addons/","title":"Similar Addons","text":"<p>Note</p> <p>This page is based on the post from @Cimbali:  https://github.com/Cimbali/CleanLinks/issues/20#issuecomment-395927561</p> Addon Removes Tracking Parameters Removes Redirects Type of Rules Notes ClearURLs yes partial GitLab/GitHub-hosted rules file Google Search Link Fix partial Site-specific: google, yandex Use unobfuscated URL present before click Google Redirects Fixer &amp; Tracking Remover yes partial Site-specific: google On request only Link Cleaner partial partial Site-specific: facebook, steam and reddit for redirects, item pages of aliexpress and amazon, utm_* parameters Don't Track Me Google partial Site-specific: google Neat URL yes LeanURL partial Parameter-specific: utm_* Pure URL partial Parameter-specific: utm_* and Site-specific: facebook, yandex Tracking Token Stripper partial Parameter-specific: utm_* Skip Redirect yes Rewriting URLs in page Remove Redirect yes Intercepting requests Open Link Directly (No Redirect) partial Site-specific: google, yahoo Untrack Me yes Not open source ? CleanLinks partial yes Parameter-specific: utm_* Referer Control Modifies HTTP referer headers Referer Modifier Modifies HTTP referer headers HTTPS Everywhere Online rules Upgrades http requests to https Consistent HTTPS Stops downgrading https to http on the same domain Canonical Manually redirects to canonical link in page (if exists: link[rel=\"canonical\"]) Universal Bypass Hardcoded Automatically redirect from URL shorteners Link Redirect Trace List/trace redirects Redirector User-defined redirects"},{"location":"specs/rules/","title":"Rule Catalogs","text":"<p>At the moment ClearURLs knows three types of rule catalogs \u2013 each in its respective file.  We will only look at two of these three catalogs, as the first version is already outdated.</p> <p>Important</p> <p>If you want to use one of these catalogs in your project and always retrieve the current version,  please use the GitLab/GitHub Pages URL to access the catalogs to preserve the GitLab/GitHub infrastructure.  The catalog files are available at:</p> GitHub (default) <ul> <li>data.minify.json: https://rules2.clearurls.xyz/data.minify.json</li> <li>rules.minify.hash: https://rules2.clearurls.xyz/rules.minify.hash</li> </ul> GitLab <ul> <li>data.minify.json: https://rules1.clearurls.xyz/data.minify.json</li> <li>rules.minify.hash: https://rules1.clearurls.xyz/rules.minify.hash</li> </ul> <p>Important</p> <p>Do not use the older data.json catalog \u2013 it is outdated and will no longer receive any updates. Instead, use the data.min.json catalog!</p>"},{"location":"specs/rules/#dataminjson-catalog","title":"data.min.json Catalog","text":"<p>The data.min.json catalog is a typical JSON file and saves all rules, exceptions, and redirections,  that are maintained by the ClearURLs developers and the community. It is structured in providers. Providers are a logical unit under which multiple rules are applied under the same <code>urlPattern</code>. Examples of providers are companies or services like Google or Reddit. Every provider has the parent element providers, it holds the reference to all providers.</p> <p>Every provider has the following properties:</p> <ul> <li><code>urlPattern</code> (required)</li> <li><code>completeProvider</code> (required)</li> <li><code>rules</code> (optional)</li> <li><code>rawRules</code> (optional)</li> <li><code>referralMarketing</code> (optional)</li> <li><code>exceptions</code> (optional)</li> <li><code>redirections</code> (optional)</li> <li><code>forceRedirection</code> (optional)</li> </ul> <p>Example of a data.min.json file</p> <pre><code>{\n    \"providers\": {\n        \"provider name\": {\n            \"urlPattern\": \"^https?://(?:[a-z0-9-]+\\\\.)*?domainName\\\\.com\",\n            \"completeProvider\": false,\n            \"rules\": [\n                \"trackingField\",\n            ],\n            \"rawRules\": [\n                \"/ref=[^/|?]*\"\n            ],\n            \"referralMarketing\": [\n                \"tag\"\n            ],\n            \"exceptions\": [\n                \"^https?://(?:[a-z0-9-]+\\\\.)*?domainName\\\\.com/re.*/redirector.html/\"\n            ],\n            \"redirections\": [\n                \"^https?://(?:[a-z0-9-]+\\\\.)*?domainName\\\\.com.*url\\\\?.*url=([^&amp;]+)\"\n            ],\n            \"forceRedirection\": false\n        }\n    }\n}</code></pre>"},{"location":"specs/rules/#urlpattern","title":"<code>urlPattern</code>","text":"<p>The urlPattern is a regular expression, that must match every URL that should be affected by the specified rules,  exceptions, or redirections of the provider.  The typical structure is <code>&lt;protocol&gt;&lt;subdomain&gt;&lt;domain name&gt;&lt;TLD&gt;&lt;directorie&gt;?&lt;fields&gt;</code>.</p> <p>In most cases the following expression covers all the needs, you only have to substitute the remaining <code>&lt;&gt;</code>  fields: <code>^https?://(?:[a-z0-9-]+\\\\.)*?&lt;domain name&gt;\\\\.&lt;TLD&gt;</code>. If you want to match with every TLD,  you can substitute the TLD field with <code>(?:[a-z]{2,}){1,}</code>.</p>"},{"location":"specs/rules/#completeprovider","title":"<code>completeProvider</code>","text":"<p>The completeProvider is a boolean, that determines if every URL that matches the urlPattern will be blocked.  If you want to specify rules, exceptions, and/or redirections, the value of completeProvider must be <code>false</code>.</p>"},{"location":"specs/rules/#rules","title":"<code>rules</code>","text":"<p>The rules property is a JSON array. Every element in this array will be automatically rewritten to  <code>(?:&amp;amp;|[/?#&amp;])(?:&lt;field name&gt;=[^&amp;]*)</code> to match the <code>field name</code>. The <code>field name</code> can still be a regular expression, but don't have to. If ClearURLs found a field that matches a rule in a given URL the field will be deleted.  Only URLs that match the url Pattern will be checked for matching fields.</p>"},{"location":"specs/rules/#rawrules","title":"<code>rawRules</code>","text":"<p>Because other elements in a URL that should be filtered, besides \"well-formed fields\" aka normal rules, there is also the rawRules property. Regular expressions formulated in this property can refer to the entire URL and not just to individual fields. They are therefore applied directly to the URL.</p>"},{"location":"specs/rules/#referralmarketing","title":"<code>referralMarketing</code>","text":"<p>Since version 1.9.0, you can allow referral marketing in ClearURLs. Previously these fields were always removed. Since some people want to support other people through referral marketing, referral marketing can be allowed in the settings. If referral marketing is allowed, the regular expressions in this array are no longer filtered.</p> <p>Attention</p> <p>Referral Marketing is disabled by default.</p>"},{"location":"specs/rules/#exceptions","title":"<code>exceptions</code>","text":"<p>The exceptions property is also a JSON array. Every element in this array is a regular expression, that matches a URL.  If ClearURLs found a URL, that matches an exception, no further processing on this URL is done.</p>"},{"location":"specs/rules/#redirections","title":"<code>redirections</code>","text":"<p>The redirections property is a JSON array. Every element in this array is a regular expression, that matches a URL and  must follow a specification.  The first regular expression group must be the URL that should be redirected.  If ClearURLs find a URL that matches the redirection, the first matching group will be decoded with will <code>decodeURIComponent()</code> and replaces the old URL.</p>"},{"location":"specs/rules/#forceredirection","title":"<code>forceRedirection</code>","text":"<p>Some websites, such as Google, have manipulated URLs (<code>&lt;a&gt;</code> tags) in such a way that the URL is no longer called normally, rather via a built-in script. The result is that a simple redirect of the URL does no longer work. Thus, to still forward the URL by ClearURLs, there is the property forceRedirection which writes the URL into the <code>main_frame</code> object of the browser.</p>"},{"location":"specs/rules/#rulesminhash","title":"rules.min.hash","text":"<p>The rules.min.hash file saves a SHA256 hash of the data.min.json file and will be automatically generated by the GitLab CI runner. It is used to ensure, that the rules are not faulty after the download. </p> <p>Important</p> <p>Note that the hash must be in lowercase.</p>"},{"location":"specs/rules/#dataminifyjson-catalog","title":"data.minify.json Catalog","text":"<p>The data.minify.json  file is automatically generated by the GitLab CI runner from the data.min.json file and also the corresponding rules.minify.hash file.  The data.minify.json is a minimal version of the data.min.json file where all line breaks and spaces,  as well as default values and empty lists, are removed to save bandwidth.</p>"}]}